---
title: "Sample Report - Data Science Capstone"
author: "Danielle Koche"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is "mehtod"?

This is an introduction to Kernel regression, which is a non-parametric
estimator that estimates the conditional expectation of two variables
which is random. The goal of a kernel regression is to discover the
non-linear relationship between two random variables. To discover the
non-linear relationship, kernel estimator or kernel smoothing is the
main method to estimate the curve for non-parametric statistics. In
kernel estimator, weight function is known as kernel function
[@efr2008]. Cite this paper [@bro2014principal]. The GEE [@wang2014].

This is my work and I want to add more work...

### Related work

This section is going to cover the literature review...

## Methods

The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.

Another equation:

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data and Visualization

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

```{r}

```

### Conclusion

## References

## Week 2 Research

Article 1:

Citation: Xiong, Z., Cui, Y., Liu, Z., Zhao, Y., Hu, M., & Hu, J. (2020). Evaluating explorative prediction power of machine learning algorithms for materials discovery using k-fold forward cross-validation. Computational Materials Science, 171, 109203.

Title: Evaluating explorative prediction power of machine learning algorithms for materials discovery using k -fold forward cross-validation

Summary: The authors of this paper have identified a need to expand upon standard k-fold cross validation approaches in their field of computational materials science. For certain materials, the authors found that predictive testing and subsequent cross validation by means of the k-fold approach is not comprehensive enough, leading the authors to expand upon k-fold cross validation in order to test samples outside of the range of the training sets used in the analysis. This paper gives a comprehensive overview of what k-fold cross validation is, and the limitations of it as seen in the need to expand upon the approach further to achieve the desired results. In this case, the desired results are a more accurate prediction of what model fits best.

---------------------------------------------------------------------------------------------------------------------------------

Article 2:

Citation: Marcot, B. G., & Hanea, A. M. (2021). What is an optimal value of k in k-fold cross-validation in discrete Bayesian network analysis?. Computational Statistics, 36(3), 2009-2031.

Title: What is an optimal value of k in k-fold cross-validation in discrete Bayesian network analysis?

Summary:  The authors of this paper provide their evidence from their study that explores the ideal value of k for k-fold cross validation analysis. The research showed that the ideal value of k is 10, however 5 can be sufficient in some cases. The paper explains the basis of k-fold cross validation and how the value of k can influence the results of the analysis, which is of extreme importance when such analyses are used to examine best fitting models.   


## Week 3 Research

Article 1:

Lei, J. (2020). Cross-validation with confidence. Journal of the American Statistical Association, 115(532), 1978-1997.

Goal of the paper: Find a solution to cross-validation techniques providing over-fitting models. The author wants to be able to test which models fit best, but the goal seems to be increasing accuracy.

Why it is important: This is important, as cross validation in itself seeks to provide a more accurate answer. This essentially is seeking to narrow the output into something even more accurate. 

How it is solved/methods: The authors solved this answer with specialized variable selection and performing subsequent linear regression analysis on the models they tested.

Results/Limitations: The authors found that they were able to produce more accurate results that were also equally understandable in the sense of understanding the interpretation of their new results. 

 

---------------------------------------------------------------------------------------------------------------------------------

Article 2:

Saud, S., Jamil, B., Upadhyay, Y., & Irshad, K. (2020). Performance improvement of empirical models for estimation of global solar radiation in India: A k-fold cross-validation approach. Sustainable Energy Technologies and Assessments, 40, 100768.

Goal of the paper: The goal of this paper was for the authors to explain how they found a method that provides more accurate predicted values using  version of k-fold cross validation analysis.

Why it is important: It is extremely important as any refinement to analytical testing can provide more accurate results. The importance is also seen in the topic in which the authors focused on, namely estimating global solar radiation levels. 

How it is solved/methods: The methods of approaching this were based on the k-fold approach which involves taking k-1 number of groups into analysis, where k = the number of predictors in a model. 

Results/Limitations: The results of this paper were a more accurate predicted level of global radiation, which the authors explain in the paper using jargon relative to that topic (which I do not fully understand to be quite honest). I understand the basis of it from a statistics perspective, but not the specific levels they were interested in analyzing due to my lack of understanding around radiation. 

---------------------------------------------------------------------------------------------------------------------------------

## Week 4 Research


Article 1:

Bates, S., Hastie, T., & Tibshirani, R. (2023). Cross-validation: what does it estimate and how well does it do it?. Journal of the American Statistical Association, 1-12.

This paper presents research on the general nature of what cross-validation accomplishes. Research is also presented that supports the hypothesis that cross validation is not always an all-encompassing technique to validate results. A more comprehensive approach to cross validation is then presented along with research that supports how this new cumulative test covers all bases compared to traditional cross-validation techniques (the paper mostly references k-fold and leave-one-out).


Article 2:

Fong, E., & Holmes, C. C. (2020). On the marginal likelihood and cross-validation. Biometrika, 107(2), 489-496.

This paper compares using the approaches of marginal likelihood against cross-validation techniques, namely the k-fold and leave-one-out approaches. Similar to the first article I have summarized for this week, the authors of this paper present their research that fills in gaps where cross-validation cannot quite cover. They essentially have developed an approach to do the task that cross-validation accomplishes with more accuracy, combining both marginal likelihood and cross-validation approaches together and presenting work that supports its increased accuracy compared to traditional approaches.
---------------------------------------------------------------------------------------------------------------------------------